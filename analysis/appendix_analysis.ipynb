{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cef82b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from collections import Counter\n",
    "import ast\n",
    "\n",
    "df = get_dataframe(True)\n",
    "\n",
    "plt.rcParams.update({'font.size': 20})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1009a00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de8e98e",
   "metadata": {},
   "source": [
    "# Appendix D.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018dbc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pointplot(x=\"sugg_accept_rate\", y=\"model_size\", data=df.query(\"interface == 'autocomplete'\"), linestyles=\"\", errorbar=\"se\")\n",
    "plt.ylabel(\"\")\n",
    "plt.xlabel(r'Autocomplete: % Suggestion Accepted')\n",
    "plt.xlim(0,0.2)\n",
    "plt.tick_params(left = False , labelleft = False ) \n",
    "plt.savefig(\"num_sugg_accepted.pdf\", format=\"pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b803cdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pointplot(x=\"sugg_accept_rate_requested\", y=\"model_size\", data=df.query(\"interface == 'autocomplete'\"), linestyles=\"\", errorbar=\"se\")\n",
    "plt.ylabel(\"\")\n",
    "plt.xlim(0, 0.5)\n",
    "plt.xlabel(r'Requested: % Suggestion Accepted')\n",
    "#plt.yticks([0,1,2], ['GPT-3.5', 'CodeLlama-34b', 'CodeLlama-7b'])\n",
    "plt.tick_params(left = False , labelleft = False ) \n",
    "plt.savefig(\"num_sugg_accepted_requested.pdf\", format=\"pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3558f18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_df = pd.read_csv(\"../data/chat_data.csv\")\n",
    "chat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93a9b4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "###chat analysis\n",
    "\n",
    "\n",
    "msg_lens = [] # length of indiv messages\n",
    "all_words = {} #words used in indiv messages\n",
    "\n",
    "\n",
    "df_chat = chat_df\n",
    "for index, row in df_chat.iterrows():\n",
    "    \n",
    "    requests = ast.literal_eval(row['request'])\n",
    "    \n",
    "    if len(requests)>0:\n",
    "        msg = requests[-1]['content']\n",
    "        \n",
    "        msg_lens.append(len(msg))\n",
    "        \n",
    "        words_in_msg = msg.split()\n",
    "            \n",
    "        for word in words_in_msg:\n",
    "            clean_word = word.lower()\n",
    "\n",
    "            if clean_word in all_words:\n",
    "                all_words[clean_word]+=1\n",
    "            else:\n",
    "                all_words[clean_word]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91ee0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "all_words_values = list(all_words.values())\n",
    "all_words_values.sort(reverse = True)\n",
    "x = [i for i in range(len(all_words_values))]\n",
    "\n",
    "plt.plot(x, all_words_values)\n",
    "plt.xticks([])\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xlabel(\"Words appearing in chat msgs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4197ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(msg_lens)\n",
    "plt.xlabel(\"Length of Chat Message\")\n",
    "plt.ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce3d0813",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg_per = [] #number of messages sent for a task\n",
    "msg_per_task = {}\n",
    "\n",
    "for programmer in chat_df['programmer_id'].unique():\n",
    "    chat_df_subset = chat_df[chat_df['programmer_id']==programmer]\n",
    "    \n",
    "    msg_per_chat = Counter(chat_df_subset['task_name'])\n",
    "    \n",
    "    for task in msg_per_chat:\n",
    "        msg_per.append(msg_per_chat[task])\n",
    "        \n",
    "        if task in msg_per_task:\n",
    "            msg_per_task[task].append(msg_per_chat[task])\n",
    "        else:\n",
    "            msg_per_task[task] = [msg_per_chat[task]]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a02df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(msg_per)\n",
    "plt.xlabel(\"Number of messages per task\")\n",
    "plt.ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb16adaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_display_order = [\"sum_product\", \n",
    "                      \"t_test\", \"table_transform_named\", \"table_transform_unnamed1\", \"table_transform_unnamed2\", \n",
    "                      \"tokenizer\", \"calculator\", \"login_authenticator\", \"retriever\",\n",
    "                      \"even_odd_count\", \"triple_sum_to_zero\", \"encode_message\", \"is_bored\", \"is_multiply_prime\", \"count_nums\", \"order_by_points\", \"event_scheduler\"]\n",
    "\n",
    "task_mapper = dict(zip(task_display_order, [\"tutorial\"]+[\"data_manipulation\"]*4+[\"edit_code\"]*4+[\"puzzles\"]*8))\n",
    "\n",
    "task_comb_results = {\"tutorial\" :[], \"data_manipulation\":[], \"edit_code\":[], \"puzzles\":[]}\n",
    "for task in msg_per_task:\n",
    "    \n",
    "    act_task = task_mapper[task]\n",
    "    task_comb_results[act_task] += msg_per_task[task]\n",
    "\n",
    "tasks = []\n",
    "msgs = []\n",
    "\n",
    "for task in task_comb_results:\n",
    "    tasks += [task]*len(task_comb_results[task])\n",
    "    msgs += task_comb_results[task]\n",
    "\n",
    "df_task_level = pd.DataFrame({\"task_category\":tasks, \"num_msg_sent\":msgs})\n",
    "df_task_level.task_category = pd.Categorical(df_task_level.task_category, \n",
    "                      categories=[\"puzzles\", \"data_manipulation\", \"edit_code\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7af563",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pointplot(y=\"task_category\", x=\"num_msg_sent\", data=df_task_level.query(\"task_category != 'tutorial'\"), linestyles=\"\", errorbar=\"se\")\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.xlabel(\"Number of messages\", fontsize=18)\n",
    "plt.ylabel(\"\")\n",
    "plt.yticks([0, 1, 2], ['Algorithmic Problems', 'Data Manipulation', 'Edit/Augment Code'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04109fd4",
   "metadata": {},
   "source": [
    "# Appendix D.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1a444aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_cols = [\"n_tasks_completed\", \"mean_task_duration\"]\n",
    "for task_id in df[\"task_id\"].unique():\n",
    "    df.loc[df[\"task_id\"] == task_id, [\"ctl_\" + x for x in outcome_cols]] = StandardScaler().fit_transform(df.loc[df[\"task_id\"] == task_id, outcome_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183af395",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pointplot(x=\"ctl_mean_task_duration\", y=\"model\", data=df, linestyles=\"\", hue=\"interface\", errorbar=\"se\")\n",
    "plt.ylabel(\"\")\n",
    "plt.xlabel(r'$\\Delta$ in Avg Task Duration')\n",
    "plt.xlim(-1, 1)\n",
    "plt.yticks([0, 1, 2,3,4,5,6,7], ['GPT-3.5-Turbo-Instruct', 'CodeLlama-34b', 'CodeLlama-7b', 'GPT-3.5-Turbo', 'GPT-4o', 'CodeLlama-34b-Instruct', 'CodeLlama-7b-Instruct', 'No LLM' ])\n",
    "plt.legend([],[], frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a115ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pointplot(x=\"ctl_n_tasks_completed\", y=\"model\", data=df, linestyles=\"\", hue=\"interface\", errorbar=\"se\")\n",
    "plt.ylabel(\"\")\n",
    "plt.xlabel(r'$\\Delta$ in Avg Task Duration')\n",
    "plt.xlim(-1, 1)\n",
    "plt.yticks([0, 1, 2,3,4,5,6,7], ['GPT-3.5-Turbo-Instruct', 'CodeLlama-34b', 'CodeLlama-7b', 'GPT-3.5-Turbo', 'GPT-4o', 'CodeLlama-34b-Instruct', 'CodeLlama-7b-Instruct', 'No LLM' ])\n",
    "plt.legend([],[], frameon=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90563869",
   "metadata": {},
   "source": [
    "# Appendix D.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "02505981",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_task_level_dfs = []\n",
    "\n",
    "import ast\n",
    "\n",
    "for row in df.itertuples():\n",
    "        \n",
    "    temp = row.task_data\n",
    "    df_temp = pd.DataFrame(temp.values()).assign(model=row.model, interface=row.interface, model_size=row.model_size, \n",
    "                                                          task_set=temp.keys())\n",
    "    list_task_level_dfs.append(df_temp)\n",
    "    \n",
    "    \n",
    "df_task_level = pd.concat(list_task_level_dfs, ignore_index=True).assign(has_ai = lambda x: x.model != \"nomodel\").query(\"time_in_task < 30*60 and name != 'event_scheduler'\").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "77772b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_display_order = [\"sum_product\", \n",
    "                      \"t_test\", \"table_transform_named\", \"table_transform_unnamed1\", \"table_transform_unnamed2\", \n",
    "                      \"tokenizer\", \"calculator\", \"login_authenticator\", \"retriever\",\n",
    "                      \"even_odd_count\", \"triple_sum_to_zero\", \"encode_message\", \"is_bored\", \"is_multiply_prime\", \"count_nums\", \"order_by_points\"]\n",
    "\n",
    "df_task_level[\"ordered_name\"] = pd.Categorical(df_task_level[\"name\"], categories=task_display_order, ordered=True)\n",
    "\n",
    "df_task_level[\"task_category\"] = df_task_level[\"name\"].map(dict(zip(task_display_order, [\"tutorial\"]+[\"data_manipulation\"]*4+[\"edit_code\"]*4+[\"puzzles\"]*7)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abcfc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,20))\n",
    "sns.pointplot(y=\"ordered_name\", x=\"time_in_task\", hue=\"has_ai\", dodge=0.25, data=df_task_level.query(\"task_category != 'tutorial'\"), linestyles=\"\", errorbar=\"se\")\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.xlabel(\"Avg task duration (s)\", fontsize=18)\n",
    "plt.ylabel(\"\")\n",
    "plt.legend(title='LLM-assisted')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b94557",
   "metadata": {},
   "source": [
    "# Appendix D.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f46ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_TLX = df[['TLX_frustration', 'TLX_performance',\n",
    "       'TLX_temporal_demand', 'TLX_physical_demand', 'TLX_effort',\n",
    "       'TLX_mental_demand','model']]\n",
    "\n",
    "df_TLX.groupby(by=[\"model\"]).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ffb587",
   "metadata": {},
   "source": [
    "# Appendix D.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f6d3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_times = []\n",
    "\n",
    "for item in df['task_completion_durations']:\n",
    "    # remove None and nan\n",
    "    item = [x for x in item if x is not None]\n",
    "    if 'nan' not in item:\n",
    "        task_times += item\n",
    "\n",
    "\n",
    "# Calculating statistics\n",
    "mean_time = np.nanmean(task_times)\n",
    "std_time = np.nanstd(task_times)\n",
    "min_time = np.nanmin(task_times)\n",
    "max_time = np.nanmax(task_times)\n",
    "median_time = np.nanmedian(task_times)\n",
    "\n",
    "# Outputting statistics for clarity\n",
    "print(f'mean time to complete task: {mean_time}')\n",
    "print(f'std time to complete task: {std_time}')\n",
    "print(f'min time to complete task: {min_time}')\n",
    "print(f'max time to complete task: {max_time}')\n",
    "print(f'median time to complete task: {median_time}')\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "n, bins, patches = ax.hist(task_times, bins='auto', color='#007acc', alpha=0.7, rwidth=0.85)\n",
    "ax.grid(axis='y', alpha=0.75)\n",
    "ax.set_xlabel('Time to Complete a Task (seconds)')\n",
    "ax.set_ylabel('Frequency')\n",
    "#ax.set_title('Histogram of Task Completion Times')\n",
    "\n",
    "# Overlaying summary statistics\n",
    "ax.axvline(mean_time, color='r', linestyle='dashed', linewidth=1)\n",
    "ax.text(mean_time, max(n)*0.97, 'Mean', rotation=0, color='r')\n",
    "\n",
    "ax.axvline(median_time, color='g', linestyle='dashed', linewidth=1)\n",
    "ax.text(median_time, max(n)*0.9, 'Median', rotation=0, color='g')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f0e637",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "times_by_model_index = {}\n",
    "max_index = 6\n",
    "model_names = df['model_name'].unique()\n",
    "for model in model_names:\n",
    "    all_data_model = df[df['model_name'] == model]['task_completion_durations'].to_list()\n",
    "    times_by_model_index[model] = []\n",
    "    for i in range(max_index):\n",
    "        list_i = []\n",
    "        for data in all_data_model:\n",
    "            if i < len(data):\n",
    "                if data[i] != None:\n",
    "                    list_i.append(data[i])\n",
    "        times_by_model_index[model].append(list_i)\n",
    "\n",
    "color0 = (0,0,0,0.8)\n",
    "color1 = (0.2,0.4,0.2,0.8) \n",
    "color2 = (0.1,0.4,0.2,1) \n",
    "color3 = (0.2,0.4,0.7,0.8) \n",
    "color4 = (0.2,0.4,0.7,1) \n",
    "color5 = (0.6,0.2,0.6,0.8) \n",
    "color6 = (0.8,0.2,0.6,1)\n",
    "color7 = (0.8,0.8,0.2,1)\n",
    "colors = [color0, color1, color2, color3, color4, color5, color6, color7]\n",
    "markers = ['o', 'x', 's', 'd', 'p', 'P', '<', '>', 'v', '^']\n",
    "for i, model in enumerate(times_by_model_index):\n",
    "    avgs = [np.nanmean(times_by_model_index[model][i]) for i in range(max_index)]\n",
    "    stds = [np.nanstd(times_by_model_index[model][i])/np.sqrt(len(times_by_model_index[model][i])) for i in range(max_index)]\n",
    "    plt.errorbar(range(max_index), avgs, yerr=stds, label=model, color=colors[i], marker=markers[i], alpha = 0.5)\n",
    "\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.get_xaxis().tick_bottom()    \n",
    "ax.get_yaxis().tick_left()   \n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.ylabel('Time to Complete Task (s) ', fontsize='xx-large')\n",
    "plt.xlabel('Task Index Solved', fontsize='xx-large')\n",
    "fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "x_labels = ['tutorial', '1', '2', '3', '4', '5']\n",
    "plt.xticks(range(max_index), x_labels, fontsize='xx-large')\n",
    "fig_size[0] = 6\n",
    "fig_size[1] = 4.2\n",
    "#plt.savefig('time_to_complete_task_index.pdf', bbox_inches='tight', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e0a007",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,14))\n",
    "sns.violinplot(x=\"zscore_mean_task_duration\", y=\"model_clean_name\", data=df, linestyles=\"\", errorbar=\"se\")\n",
    "plt.ylabel(\"\")\n",
    "plt.xlabel(r'$\\Delta$ in Avg Task Duration ($\\downarrow$ better)')\n",
    "#plt.xlim(-1, 1)\n",
    "#plt.xlim(-120,100)\n",
    "#plt.yticks([0,1,2,3], ['GPT-3.5', 'CodeLlama-34b', 'CodeLlama-7b', 'No LLM'])#plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.legend([],[], frameon=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d44274f",
   "metadata": {},
   "source": [
    "# Edit Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b49eedd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "autocomplete_df = pd.read_csv(\"../data/autocomplete_data.csv\")\n",
    "study_df = pd.read_csv(\"../data/study_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589fe5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add column to autocomplete_df which is the pd.DataFrame(dataset['study'][programmer_id]['code_history'])\n",
    "autocomplete_df['code_history'] = None\n",
    "for i in tqdm(range(len(autocomplete_df))):\n",
    "    programmer_id = int(autocomplete_df['programmer_id'].iloc[i])\n",
    "    task_name = autocomplete_df['task_name'].iloc[i]\n",
    "    timestamp = autocomplete_df['timestamp'].iloc[i] \n",
    "    code_history = pd.read_json(study_df.iloc[programmer_id]['code_history'])\n",
    "    code_history = code_history[code_history['task_name'] == task_name]\n",
    "    # only keep the code history that was written after the timestamp\n",
    "    #print(timestamp)\n",
    "    # make sure code_history['times'] is integer\n",
    "    code_history['times'] = code_history['times'].astype(int)\n",
    "    # in code_history how many times the code was written before the timestamp\n",
    "    code_history = code_history[code_history['times'] > timestamp]\n",
    "    suffix = autocomplete_df['suffix_code'].iloc[i] \n",
    "    prefix = autocomplete_df['prefix_code'].iloc[i]\n",
    "    # if suffix nan\n",
    "    if suffix != suffix:\n",
    "        suffix = \"\"\n",
    "    if prefix != prefix:\n",
    "        prefix = \"\"\n",
    "    accepted = autocomplete_df['accepted'].iloc[i]\n",
    "    if accepted:\n",
    "        code_at_acceptance = prefix + autocomplete_df['suggestion'].iloc[i] + suffix\n",
    "    else:\n",
    "        code_at_acceptance = prefix + suffix\n",
    "    code_at_shown = prefix + autocomplete_df['suggestion'].iloc[i] + suffix\n",
    "    \n",
    "    if not accepted:\n",
    "        for j in range(len(code_history)):\n",
    "            code_j = code_history['code'].iloc[j]\n",
    "            if code_j == code_at_shown:\n",
    "                code_history = code_history.iloc[j+1:]\n",
    "                break\n",
    "        \n",
    "    new_row = {'code': code_at_acceptance, 'times': 0, 'time_gaps': 0, 'edit_score': None}\n",
    "    #re order code history so last row becomes first\n",
    "    code_history = pd.concat([pd.DataFrame(new_row, index=[0]), code_history], ignore_index=True)\n",
    "    code_history['edit_score'] = None\n",
    "    row_index = i  # Index of the row\n",
    "    col_index = autocomplete_df.columns.get_loc('code_history')  # Get the integer index of the column\n",
    "    autocomplete_df.iat[row_index, col_index] = code_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8e3e454a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_included(row):\n",
    "    suggestion = row['suggestion']\n",
    "    timestamp = row['timestamp']\n",
    "    code_history = row['code_history']\n",
    "    accepted = row['accepted']\n",
    "    if accepted == 0:\n",
    "        return -1\n",
    "    # Filter code_history where times > timestamp\n",
    "    filtered_history = code_history[code_history['times'] > timestamp]\n",
    "    len_filtered = len(filtered_history)\n",
    "    if not filtered_history.empty:\n",
    "        # Check if suggestion is in the first 'code' entry\n",
    "        first_code = filtered_history.iloc[min(min_indx,len_filtered-1)]['code']\n",
    "        return 1 if suggestion in first_code else 0\n",
    "    \n",
    "    return -1\n",
    "\n",
    "# Apply the function to each row\n",
    "min_indx = 0 \n",
    "autocomplete_df['in_code_after_15s'] = autocomplete_df.apply(check_included, axis=1)\n",
    "min_indx = 1\n",
    "autocomplete_df['in_code_after_30s'] = autocomplete_df.apply(check_included, axis=1)\n",
    "min_indx = 3\n",
    "autocomplete_df['in_code_after_60s'] = autocomplete_df.apply(check_included, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ac983d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filtered_df = autocomplete_df[autocomplete_df['in_code_after_15s'] != -1]\n",
    "\n",
    "average_by_model = filtered_df.groupby('model')['in_code_after_15s'].mean().reset_index()\n",
    "\n",
    "# Print the result\n",
    "print(average_by_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921f8c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filtered_df = autocomplete_df[autocomplete_df['in_code_after_30s'] != -1]\n",
    "\n",
    "average_by_model = filtered_df.groupby('model')['in_code_after_30s'].mean().reset_index()\n",
    "\n",
    "# Print the result\n",
    "print(average_by_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59bed15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filtered_df = autocomplete_df[autocomplete_df['in_code_after_60s'] != -1]\n",
    "\n",
    "average_by_model = filtered_df.groupby('model')['in_code_after_60s'].mean().reset_index()\n",
    "\n",
    "# Print the result\n",
    "print(average_by_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a63c125",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "# Data\n",
    "time_after_acceptance = [15, 30, 60]\n",
    "models = [ 'CodeLlama7b', 'CodeLlama34b', 'GPT-3.5']\n",
    "\n",
    "in_code_after_15s = [ 0.797468, 0.845238, 0.835777]\n",
    "in_code_after_30s = [ 0.398734, 0.428571, 0.480938]\n",
    "in_code_after_60s = [ 0.158228, 0.321429, 0.313783]\n",
    "\n",
    "# Plotting\n",
    "\n",
    "plt.plot(time_after_acceptance, [in_code_after_15s[1], in_code_after_30s[1], in_code_after_60s[1]], marker='o', label='CodeLlama34b')\n",
    "plt.plot(time_after_acceptance, [in_code_after_15s[0], in_code_after_30s[0], in_code_after_60s[0]], marker='o', label='CodeLlama7b')\n",
    "plt.plot(time_after_acceptance, [in_code_after_15s[2], in_code_after_30s[2], in_code_after_60s[2]], marker='o', label='GPT-3.5')\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.xlabel('Time after Acceptance (s)')\n",
    "plt.ylabel('Still in Code Fraction')\n",
    "\n",
    "# Adding a legend\n",
    "plt.legend(title='Models')\n",
    "\n",
    "# Display the plot\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
