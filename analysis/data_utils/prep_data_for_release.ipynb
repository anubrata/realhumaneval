{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "pd.set_option('display.max_columns', None)\n",
    "import json\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "import statsmodels.formula.api as smf \n",
    "from IPython.display import display, Markdown\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"../../data/final_df.pkl\")\n",
    "df[\"model_size\"] = [x.split(\"_\")[1] if x != \"nomodel\" else \"nomodel\" for x in df[\"model\"]]\n",
    "model_name_mapping = {\n",
    "    'nomodel': 'No LLM',  # Assuming an empty string or some default value might be appropriate\n",
    "    'chat_gpt35': 'GPT-3.5 (chat)',\n",
    "    'autocomplete_gpt35': 'GPT-3.5',\n",
    "    'autocomplete_llama34': 'CodeLlama34b',\n",
    "    'chat_llama7': 'CodeLlama7b (chat)',\n",
    "    'autocomplete_llama7': 'CodeLlama7b',\n",
    "    'chat_llama34': 'CodeLlama34b (chat)',\n",
    "    'chat_gpt4': 'GPT-4o (chat)',\n",
    "}\n",
    "\n",
    "df['model_name'] = df['model'].map(model_name_mapping)\n",
    "# drop both_gpt35 model df\n",
    "df = df[df[\"model\"] != \"both_gpt35\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['model'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autocomplete csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"prefix_code\", \"suffix_code\", \"suggestion\", \"logprobs\", \"accepted\", \"programmer_id\", \"timestamp\", \"model\", \"task_name\", \"requested\"]\n",
    "\n",
    "# Create an empty DataFrame with the defined columns\n",
    "autocomplete_df = pd.DataFrame(columns=columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "programmer_ids = []\n",
    "prefixes = []\n",
    "suffixes = []\n",
    "suggestions = []\n",
    "logprobs = []\n",
    "accepts = []\n",
    "timestamps = []\n",
    "models = []\n",
    "task_names = []\n",
    "requested = []\n",
    "for i in range(len(df)):\n",
    "    curr_df = df['suggestions_data'].iloc[i]\n",
    "    if len(curr_df) == 0:\n",
    "        continue\n",
    "    initial_timestamp = curr_df['times'].iloc[0]\n",
    "    for j in range(len(curr_df)):\n",
    "        programmer_ids.append(i)\n",
    "        models.append(df['model_name'].iloc[i])\n",
    "        requested.append(curr_df['requested'].iloc[j])\n",
    "        task_names.append(curr_df['task_name'].iloc[j])\n",
    "        prefixes.append(curr_df['prefix_code'].iloc[j])\n",
    "        suffixes.append(curr_df['suffix_code'].iloc[j])\n",
    "        suggestions.append(curr_df['suggestion'].iloc[j])\n",
    "        logprobs.append(curr_df['logprobs'].iloc[j])\n",
    "        accepts.append(curr_df['label'].iloc[j])\n",
    "        #timestamps.append((curr_df['times'].iloc[j]- initial_timestamp)/1000)\n",
    "        timestamps.append((curr_df['times'].iloc[j])/1)\n",
    "autocomplete_df['programmer_id'] = programmer_ids\n",
    "autocomplete_df['prefix_code'] = prefixes\n",
    "autocomplete_df['suffix_code'] = suffixes\n",
    "autocomplete_df['suggestion'] = suggestions\n",
    "autocomplete_df['logprobs'] = logprobs\n",
    "autocomplete_df['accepted'] = accepts\n",
    "autocomplete_df['timestamp'] = timestamps\n",
    "autocomplete_df['model'] = models\n",
    "autocomplete_df['task_name'] = task_names\n",
    "autocomplete_df['requested'] = requested\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv\n",
    "autocomplete_df.to_csv(\"../../data/autocomplete_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"request\", \"response\", \"copy_events\", \"logprobs\", \"programmer_id\", \"timestamp\", \"model\", \"task_name\"]\n",
    "\n",
    "# Create an empty DataFrame with the defined columns\n",
    "chat_df = pd.DataFrame(columns=columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "programmer_ids = []\n",
    "requests = []\n",
    "responses = []\n",
    "copy_events = []\n",
    "timestamps = []\n",
    "models = []\n",
    "logprobs = []\n",
    "task_names = []\n",
    "for i in range(len(df)):\n",
    "    curr_df = df['chat_history_data'].iloc[i]\n",
    "    if len(curr_df) == 0:\n",
    "        continue\n",
    "    initial_timestamp = curr_df['times'].iloc[0]\n",
    "    for j in range(len(curr_df)):\n",
    "        programmer_ids.append(i)\n",
    "        models.append(df['model_name'].iloc[i])\n",
    "        task_names.append(curr_df['task_name'].iloc[j])\n",
    "        requests.append(curr_df['message'].iloc[j])\n",
    "        responses.append(curr_df['response'].iloc[j])\n",
    "        copy_events.append(curr_df['copy_info'].iloc[j])\n",
    "        logprobs.append(curr_df['logprobs'].iloc[j])\n",
    "        timestamps.append((curr_df['times'].iloc[j]- initial_timestamp)/1000)\n",
    "\n",
    "\n",
    "chat_df['programmer_id'] = programmer_ids\n",
    "chat_df['request'] = requests\n",
    "chat_df['response'] = responses\n",
    "chat_df['copy_events'] = copy_events\n",
    "chat_df['timestamp'] = timestamps\n",
    "chat_df['model'] = models\n",
    "chat_df['task_name'] = task_names\n",
    "chat_df['logprobs'] = logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv\n",
    "chat_df.to_csv(\"../../data/chat_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tasks\n",
    "import os, json\n",
    "folder_path = '../../tasks_study/tasks'  \n",
    "all_tasks = []\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith('.json'):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        with open(file_path, 'r') as file:\n",
    "            task_data = json.load(file)\n",
    "            all_tasks.append(task_data)\n",
    "\n",
    "all_tasks = np.array(all_tasks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"name\", \"task_description\", \"function_signature\" , \"unit_test\", \"solution\", \"type\" ]\n",
    "tasks_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "for i in range(len(all_tasks)):\n",
    "    tasks_df = tasks_df.append({\n",
    "        \"name\": all_tasks[i][\"name\"],\n",
    "        \"task_description\": all_tasks[i][\"task_description\"],\n",
    "        \"function_signature\": all_tasks[i][\"function_signature\"],\n",
    "        \"unit_test\": all_tasks[i][\"unit_test\"],\n",
    "        \"solution\": all_tasks[i][\"solution\"],\n",
    "        \"type\": all_tasks[i][\"type\"]\n",
    "    }, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv\n",
    "tasks_df.to_csv(\"../../data/tasks_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Telemetry csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy df\n",
    "santized_df = df.copy()\n",
    "santized_df = santized_df.drop(columns=['level_0', 'index', 'finalcomments', 'exp_condition', 'date_performed', 'completed_task' , 'TLX_total_score',  'howaiimproved', 'frustration', 'performance', 'howaiimproved', 'worker_id', 'temporalDemand', 'telemetry_data', 'mentalDemand', 'completed_task_time',  'entered_exit_survey', 'effort', 'physicalDemand', 'howaihelpful', 'time_completed', 'task_index', 'test', 'suggestions_data', 'chat_history_data', 'n_long_gaps', 'n_participants'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#santized_df.iloc[172]['task_data'] \n",
    "# BEGIN: santized_df.iloc[172]['task_data']\n",
    "for key, value in santized_df.iloc[172]['task_data'].items():\n",
    "    santized_df.iloc[172]['task_data'][key]['code'] = \"\"\n",
    "    print( santized_df.iloc[172]['task_data'][key]['code'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add column programmer_id as index\n",
    "santized_df['programmer_id'] = santized_df.index\n",
    "#santized_df['code_history'] = santized_df['code_history'].apply(lambda x: x.assign(times=(x['times'] - x['times'].iloc[0])/1000))\n",
    "\n",
    "# task_completion_durations, code_history, task_data\n",
    "santized_df['task_completion_durations'] = santized_df['task_completion_durations'].apply(lambda x: [None if pd.isna(y) else y for y in x])\n",
    "\n",
    "santized_df['code_history'] = santized_df['code_history'].apply(lambda x: x.to_json())\n",
    "# Convert the 'task_data' dictionary column to a JSON string\n",
    "#santized_df['task_data'] = santized_df['task_data'].apply(json.dumps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv\n",
    "santized_df.to_csv(\"../../data/study_data.csv\", index=False)\n",
    "# save to pickle\n",
    "santized_df.to_pickle(\"../../data/study_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from csv\n",
    "santized_df = pd.read_csv(\"../../data/study_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "santized_df['code_history'] = santized_df['code_history'].apply(lambda x: pd.read_json(x))\n",
    "#santized_df['task_data'] = santized_df['task_data'].apply(json.loads)\n",
    "santized_df['task_completion_durations'] = santized_df['task_completion_durations'].map(lambda x: eval(x))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hussein2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
