{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import firebase_admin\n",
    "from firebase_admin import credentials\n",
    "from firebase_admin import firestore\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import re \n",
    "import ast \n",
    "import dateutil.parser as dparser\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "import math\n",
    "from datetime import datetime, timedelta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import firebase_admin\n",
    "from firebase_admin import credentials\n",
    "from firebase_admin import firestore\n",
    "\n",
    "cred = credentials.Certificate(\"../../codeinterface-85b5e-firebase-adminsdk-11q7e-837ba92a03.json\")\n",
    "firebase_admin.initialize_app(cred)\n",
    "db = firestore.client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Helper functions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get actual tasks participants solved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tasks\n",
    "folder_path = '../../tasks_study/tasks'  \n",
    "all_tasks = []\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith('.json'):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        with open(file_path, 'r') as file:\n",
    "            task_data = json.load(file)\n",
    "            all_tasks.append(task_data)\n",
    "\n",
    "all_tasks = np.array(all_tasks)\n",
    "def get_task_index(task_name):\n",
    "    for i in range(len(all_tasks)):\n",
    "        if all_tasks[i]['name'] == task_name:\n",
    "            return i\n",
    "    raise ValueError('Task not found')\n",
    "tutorial_task_index = get_task_index('sum_product')\n",
    "\n",
    "task_sets = [np.array([ get_task_index('even_odd_count'), get_task_index('triple_sum_to_zero'),   get_task_index('table_transform_named'), get_task_index('tokenizer'),\n",
    "                          get_task_index('encode_message'), get_task_index('t_test'), get_task_index('event_scheduler')]),\n",
    "\n",
    "            np.array([ get_task_index('even_odd_count'), get_task_index('is_bored'),   get_task_index('login_authenticator'), get_task_index('is_multiply_prime'),\n",
    "                          get_task_index('count_nums'), get_task_index('table_transform_named'), get_task_index('calculator')]),\n",
    "\n",
    "            np.array([ get_task_index('even_odd_count'), get_task_index('count_nums'),   get_task_index('calculator'), get_task_index('table_transform_unnamed2'),\n",
    "                          get_task_index('login_authenticator'), get_task_index('encode_message'), get_task_index('is_bored')]),\n",
    "\n",
    "            np.array([ get_task_index('even_odd_count'), get_task_index('order_by_points'),   get_task_index('retriever'), get_task_index('triple_sum_to_zero'),\n",
    "                          get_task_index('tokenizer'), get_task_index('event_scheduler'), get_task_index('encode_message')]),\n",
    "\n",
    "            np.array([ get_task_index('even_odd_count'), get_task_index('is_multiply_prime'),   get_task_index('table_transform_unnamed1'), get_task_index('t_test'),\n",
    "                          get_task_index('is_bored'), get_task_index('order_by_points'), get_task_index('triple_sum_to_zero')])]           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_suggestion_acceptance_rate(telemetry_data):\n",
    "    num_accept = len([event for event in telemetry_data if event[\"event_type\"] == \"accept\"])\n",
    "    # only count suggestion_shown when suggestion is not \"\"\n",
    "    num_suggestion_shown = len([event for event in telemetry_data if event[\"event_type\"] == \"suggestion_shown\" and event[\"suggestion\"] != \"\"])\n",
    "    suggestion_ids_shown = [event[\"suggestion_id\"] for event in telemetry_data if event[\"event_type\"] == \"suggestion_shown\" and event[\"suggestion\"] != \"\"]\n",
    "    suggestion_log_probs = [event[\"logprobs\"]['firstElement'] for event in telemetry_data if event[\"event_type\"] == \"suggestion_shown\" and event[\"suggestion\"] != \"\"]\n",
    "    sugggestion_ids_accepted = [event[\"suggestion_id\"] for event in telemetry_data if event[\"event_type\"] == \"accept\"]\n",
    "    sugggestion_ids_rejected = [event[\"suggestion_id\"] for event in telemetry_data if event[\"event_type\"] == \"reject\"]\n",
    "    accepted = 0\n",
    "    shown_true = 0\n",
    "\n",
    "    for suggestion_id in suggestion_ids_shown:\n",
    "        if suggestion_id in sugggestion_ids_accepted:\n",
    "            accepted += 1\n",
    "        if suggestion_id in sugggestion_ids_rejected or suggestion_id in sugggestion_ids_accepted:\n",
    "            shown_true += 1\n",
    "    if shown_true == 0:\n",
    "        return 0, 0\n",
    "    return accepted, shown_true\n",
    "\n",
    "\n",
    "def get_suggestions_labels(telemetry_data, task_id):\n",
    "    num_accept = len([event for event in telemetry_data if event[\"event_type\"] == \"accept\"])\n",
    "    # only count suggestion_shown when suggestion is not \"\"\n",
    "    num_suggestion_shown = len([event for event in telemetry_data if event[\"event_type\"] == \"suggestion_shown\" and event[\"suggestion\"] != \"\"])\n",
    "    suggestion_ids_shown = [event[\"suggestion_id\"] for event in telemetry_data if event[\"event_type\"] == \"suggestion_shown\" and event[\"suggestion\"] != \"\"]\n",
    "    suggestion_shown = [event for event in telemetry_data if event[\"event_type\"] == \"suggestion_shown\" and event[\"suggestion\"] != \"\"]\n",
    "    suggestions_before_shown = [event for event in telemetry_data if event[\"event_type\"] == \"before_shown\"]\n",
    "    suggestions_before_shown_ids = [event[\"suggestion_id\"] for event in telemetry_data if event[\"event_type\"] == \"before_shown\"]\n",
    "    suggestion_log_probs = [event[\"logprobs\"] for event in telemetry_data if event[\"event_type\"] == \"suggestion_shown\" and event[\"suggestion\"] != \"\"]\n",
    "    sugggestion_ids_accepted = [event[\"suggestion_id\"] for event in telemetry_data if event[\"event_type\"] == \"accept\"]\n",
    "    sugggestion_ids_rejected = [event[\"suggestion_id\"] for event in telemetry_data if event[\"event_type\"] == \"reject\"]\n",
    "    suggestions_ids_requested = [event[\"suggestion_id\"] for event in telemetry_data if event[\"event_type\"] == \"request_suggestion\"]\n",
    "\n",
    "    accepted = 0\n",
    "    shown_true = 0\n",
    "    suggestion_labels = []\n",
    "    suggestion_probs = []\n",
    "    suggestions_text = []\n",
    "    suggestions_prefix = []\n",
    "    suggestions_suffix = []\n",
    "    suggestions_task_names = []\n",
    "    suggestions_times = []\n",
    "    suggestions_requested = []\n",
    "    task_set_user = task_sets[task_id]\n",
    "    for suggestion_id in suggestion_ids_shown:\n",
    "        if suggestion_id not in sugggestion_ids_rejected and suggestion_id not in sugggestion_ids_accepted:\n",
    "            continue\n",
    "        suggestions_times.append(suggestion_shown[suggestion_ids_shown.index(suggestion_id)][\"timestamp\"])\n",
    "        suggestion_probs.append(suggestion_log_probs[suggestion_ids_shown.index(suggestion_id)])\n",
    "        suggestions_text.append(suggestion_shown[suggestion_ids_shown.index(suggestion_id)][\"suggestion\"])\n",
    "        task_index = suggestion_shown[suggestion_ids_shown.index(suggestion_id)][\"task_index\"]\n",
    "        suggestions_requested.append(suggestion_id-1 in suggestions_ids_requested)\n",
    "        if task_index == -1:\n",
    "            task_name = all_tasks[tutorial_task_index][\"name\"]\n",
    "        else:\n",
    "            task_name = all_tasks[task_set_user[task_index]][\"name\"]\n",
    "        suggestions_task_names.append(task_name)\n",
    "        # prefix_code and suffix_code are found in before_shown\n",
    "        suggestions_prefix.append(suggestions_before_shown[suggestions_before_shown_ids.index(suggestion_id)][\"prefix_code\"])\n",
    "        suggestions_suffix.append(suggestions_before_shown[suggestions_before_shown_ids.index(suggestion_id)][\"suffix_code\"])\n",
    "        if suggestion_id in sugggestion_ids_accepted:\n",
    "            suggestion_labels.append(1)\n",
    "        else:\n",
    "            suggestion_labels.append(0)\n",
    "    suggestions_info = pd.DataFrame({'suggestion': suggestions_text, 'prefix_code': suggestions_prefix,\n",
    "     'suffix_code': suggestions_suffix, 'label': suggestion_labels, 'logprobs': suggestion_probs, 'times': suggestions_times, 'task_name': suggestions_task_names, 'requested': suggestions_requested})\n",
    "    return suggestions_info\n",
    "\n",
    "def get_suggestions_requested(telemetry_data):\n",
    "    suggestion_ids_shown = [event[\"suggestion_id\"] for event in telemetry_data if event[\"event_type\"] == \"suggestion_shown\" and event[\"suggestion\"] != \"\"]\n",
    "    sugggestion_ids_accepted = [event[\"suggestion_id\"] for event in telemetry_data if event[\"event_type\"] == \"accept\"]\n",
    "    sugggestion_ids_rejected = [event[\"suggestion_id\"] for event in telemetry_data if event[\"event_type\"] == \"reject\"]\n",
    "    # request_suggestion\n",
    "    suggestions_requested = [event[\"suggestion_id\"] for event in telemetry_data if event[\"event_type\"] == \"request_suggestion\"]\n",
    "\n",
    "    n_req = 0\n",
    "    n_accepted = 0\n",
    "    for id in suggestions_requested:\n",
    "        if id+1 not in suggestion_ids_shown:\n",
    "            continue\n",
    "        if id+1 not in sugggestion_ids_accepted and id+1 not in sugggestion_ids_rejected:\n",
    "            continue\n",
    "        if id+1 in sugggestion_ids_accepted:\n",
    "            n_accepted += 1\n",
    "        n_req += 1\n",
    "    return n_req, n_accepted\n",
    "\n",
    "\n",
    "def get_time_to_completion(telemetry_data):\n",
    "    starts = [event for event in telemetry_data if event[\"event_type\"] == \"load_task\"]\n",
    "    ends = [\n",
    "        event#[\"timestamp\"]\n",
    "        for event in telemetry_data\n",
    "        if event[\"event_type\"] == \"submit_code\" and event[\"completed_task\"] == 1\n",
    "    ]\n",
    "    times_tasks_solved = []\n",
    "    task_indices_seen = set()\n",
    "    for start in starts:\n",
    "        if start[\"task_index\"] in task_indices_seen:\n",
    "            continue\n",
    "        task_indices_seen.add(start[\"task_index\"])\n",
    "        max_time = 1800\n",
    "        min_time = max_time\n",
    "        for end in ends:\n",
    "            if end[\"task_index\"] == start[\"task_index\"]: \n",
    "                # check if tim is more than 10mins\n",
    "                if (end[\"timestamp\"] - start[\"timestamp\"]) / 1000 < min_time:\n",
    "                    times_tasks_solved.append((end[\"timestamp\"] - start[\"timestamp\"]) / 1000)\n",
    "                    break\n",
    "        if min_time < max_time:\n",
    "            times_tasks_solved.append(min_time)\n",
    "\n",
    "    if len(times_tasks_solved) == 0:\n",
    "        return [np.nan], np.nan\n",
    "    return times_tasks_solved, np.mean(times_tasks_solved)\n",
    "\n",
    "\n",
    "def get_coding_time(telemetry, entered_exit_survey, date_performed):\n",
    "    # Get first load task\n",
    "    # get first event in telemetry timestamp and last event in telemetry timestamp \n",
    "    # get time difference\n",
    "    time1_telem = telemetry[0][\"timestamp\"]\n",
    "    time2_telem = telemetry[-1][\"timestamp\"]\n",
    "    diff_telemetry = (time2_telem - time1_telem) / 1000\n",
    "\n",
    "    time2 = entered_exit_survey\n",
    "    time1 = date_performed\n",
    "    time2 = time2.split(\" \")[4]\n",
    "    # day of time2 is same as time1 but time2 only has time without time zone\n",
    "    time2 = datetime.strptime(time2, '%H:%M:%S')\n",
    "    # only keep time without day or timezone in time1\n",
    "    time1 = time1.strftime('%H:%M:%S')\n",
    "    time1 = datetime.strptime(time1, '%H:%M:%S')\n",
    "    time_diff = time2 - time1\n",
    "    time_diff = time_diff.seconds\n",
    "    # if time_diff is more than 36 minutes, use telemetry time\n",
    "    if time_diff > 36*60:\n",
    "        return diff_telemetry\n",
    "    else:\n",
    "        return time_diff\n",
    "\n",
    "\n",
    "def get_time_verifying_suggestion(telemetry_data):\n",
    "    # Get suggestions\n",
    "    suggestions_shown = [event for event in telemetry_data if event[\"event_type\"] == \"suggestion_shown\"]\n",
    "    suggestions_reviewed = [\n",
    "        event for event in telemetry_data if event[\"event_type\"] == \"reject\" or event[\"event_type\"] == \"accept\"\n",
    "    ]\n",
    "\n",
    "    # Create a hashmap for suggestion reviews.\n",
    "    reviewed_hashmap = {}\n",
    "    for event in suggestions_reviewed:\n",
    "        reviewed_hashmap[event[\"suggestion_id\"]] = event[\"timestamp\"]\n",
    "\n",
    "    # Create a hashmap for times to completion\n",
    "    time_spent_verifying = {}\n",
    "    for event in suggestions_shown:\n",
    "        if event[\"suggestion_id\"] in reviewed_hashmap:\n",
    "            time_spent_verifying[event[\"suggestion_id\"]] = (\n",
    "                reviewed_hashmap[event[\"suggestion_id\"]] - event[\"timestamp\"]\n",
    "            ) / 1000\n",
    "        # else:\n",
    "        #     print(\"No review found for suggestion: \", event[\"suggestion_id\"])\n",
    "\n",
    "    return time_spent_verifying\n",
    "\n",
    "\n",
    "def get_chat_messages_count(telemetry_data):\n",
    "    assistant_response_count = len([event for event in telemetry_data if event[\"event_type\"] == \"assistant_response\"])\n",
    "    user_message_count = len([event for event in telemetry_data if event[\"event_type\"] == \"user_message\"])\n",
    "    copy_code_button_count = len([event for event in telemetry_data if event[\"event_type\"] == \"copy_code\"])\n",
    "    copy_from_chat_count = len([event for event in telemetry_data if event[\"event_type\"] == \"copy_from_chat\"])\n",
    "    # for each assistant response, find a copy_code or copy_from_chat with same messageAIindex and where copied_text is a substring of response in assistant_response\n",
    "    suggestions_copied = []\n",
    "    for event in telemetry_data:\n",
    "        if event[\"event_type\"] == \"assistant_response\":\n",
    "            response = event[\"response\"]\n",
    "            messageAIindex = event[\"messageAIindex\"]\n",
    "            count = 0\n",
    "            for copy_event in telemetry_data:\n",
    "                # check timestamps order\n",
    "                if copy_event['timestamp'] < event['timestamp']:\n",
    "                    continue\n",
    "                if copy_event['task_index'] != event['task_index']:\n",
    "                    continue\n",
    "                if copy_event[\"event_type\"] == \"copy_code\" and copy_event[\"copied_text\"] in response:\n",
    "                    count += 1\n",
    "                if copy_event[\"event_type\"] == \"copy_from_chat\"  and copy_event[\"copied_text\"] in response:\n",
    "                    count += 1\n",
    "            count = min(count, 1)\n",
    "            suggestions_copied.append(count)\n",
    "    avg_copy_per_response = np.nanmean(suggestions_copied)\n",
    "    return assistant_response_count, user_message_count, copy_code_button_count, copy_from_chat_count, avg_copy_per_response\n",
    "\n",
    "def get_task_data(telemetry_data, task_id):\n",
    "    task_set_user = task_sets[task_id]\n",
    "    task_data = {}\n",
    "    load_events = [event for event in telemetry_data if event[\"event_type\"] == \"load_task\"]\n",
    "    skip_events = [event for event in telemetry_data if event[\"event_type\"] == \"skip_task\"]\n",
    "    submit_events = [event for event in telemetry_data if event[\"event_type\"] == \"submit_code\"]\n",
    "    # save code events\n",
    "    save_events = [event for event in telemetry_data if event[\"event_type\"] == \"save_code\"]\n",
    "    for i in range(len(load_events)):\n",
    "        task_index = load_events[i][\"task_index\"]\n",
    "        if task_index == -1:\n",
    "            task_name = all_tasks[tutorial_task_index][\"name\"]\n",
    "        else:\n",
    "            task_name = all_tasks[task_set_user[task_index]][\"name\"]\n",
    "        if task_index in task_data:\n",
    "            continue\n",
    "        time_in_task = 0 # time to the next load task with index+1\n",
    "        completed = False # if there exists a submit event with same task_index and completed_task=1\n",
    "        skipped = False\n",
    "        code = \"\" # get the save_event with same task_index and the latest timestamp\n",
    "        for j in range(i+1, len(load_events)):\n",
    "            if load_events[j][\"task_index\"] == task_index+1:\n",
    "                time_in_task = (load_events[j][\"timestamp\"] - load_events[i][\"timestamp\"]) / 1000\n",
    "                break\n",
    "        for submit_event in submit_events:\n",
    "            if submit_event[\"task_index\"] == task_index and submit_event[\"completed_task\"] == 1:\n",
    "                completed = True\n",
    "                break\n",
    "        for save_event in save_events:\n",
    "            if save_event[\"task_index\"] == task_index:\n",
    "                code = save_event[\"code\"]\n",
    "                \n",
    "        for skip_event in skip_events:\n",
    "            if skip_event[\"task_index\"] == task_index:\n",
    "                skipped = True\n",
    "                break\n",
    "        task_data[task_index] = {'name': task_name, 'time_in_task': time_in_task, 'completed': completed, 'code': code, 'skipped': skipped}\n",
    "    return task_data\n",
    "\n",
    "\n",
    "def get_chat_history(telemetry_data, task_id):\n",
    "    # get user_message and assistant_response events\n",
    "    user_messages = [event for event in telemetry_data if event[\"event_type\"] == \"user_message\"]\n",
    "    assistant_responses = [event for event in telemetry_data if event[\"event_type\"] == \"assistant_response\"]\n",
    "    clear_chat = [event for event in telemetry_data if event[\"event_type\"] == \"clear_chat\"]\n",
    "    copy_code = [event for event in telemetry_data if event[\"event_type\"] == \"copy_code\"]\n",
    "    copy_from_chat = [event for event in telemetry_data if event[\"event_type\"] == \"copy_from_chat\"]\n",
    "    # df: input, aii response, logprobs, copy events and what was copied, taskname, times\n",
    "    chat_responses = []\n",
    "    chat_messages = []\n",
    "    chat_logprobs = []\n",
    "    chat_times = []\n",
    "    chat_tasks = []\n",
    "    chat_copy_info = []\n",
    "    for response in assistant_responses:\n",
    "        chat_responses.append(response[\"response\"])\n",
    "        chat_logprobs.append(response[\"logprobs\"])\n",
    "        chat_times.append(response[\"timestamp\"])\n",
    "        history = response[\"chatHistory\"]\n",
    "        history_proccessed = []\n",
    "        for msg in history:\n",
    "            if msg['role'] == 'system':\n",
    "                continue\n",
    "            if msg['role'] == 'assistant' and msg['content'] == response[\"response\"]:\n",
    "                break\n",
    "            history_proccessed.append(msg)\n",
    "        chat_messages.append(history_proccessed)\n",
    "\n",
    "        if response[\"task_index\"] == -1:\n",
    "            task_name = all_tasks[tutorial_task_index][\"name\"]\n",
    "        else:\n",
    "            task_name = all_tasks[task_sets[task_id][response[\"task_index\"]]][\"name\"]\n",
    "        chat_tasks.append(task_name)\n",
    "        copy_info = []\n",
    "        for copy_event in copy_code:\n",
    "            copied_text = copy_event[\"copied_text\"]\n",
    "            # if copy event has same taskindex and timestamp is before response timestamp\n",
    "            if copy_event[\"task_index\"] == response[\"task_index\"] and copy_event[\"timestamp\"] >= response[\"timestamp\"]:\n",
    "                # check if copied_text is found inside response\n",
    "                if copied_text in response[\"response\"]:\n",
    "                    copy_info.append({'type': 'copy_button', 'copied': copied_text})\n",
    "        for copy_event in copy_from_chat:\n",
    "            copied_text = copy_event[\"copied_text\"]\n",
    "            if copy_event[\"task_index\"] == response[\"task_index\"] and copy_event[\"timestamp\"] >= response[\"timestamp\"]:\n",
    "                if copied_text in response[\"response\"]:\n",
    "                    copy_info.append({'type': 'copy_chat', 'copied': copied_text})\n",
    "        chat_copy_info.append(copy_info)\n",
    "    messages_df = pd.DataFrame({'message': chat_messages, 'response': chat_responses, 'logprobs': chat_logprobs, 'times': chat_times, 'task_name': chat_tasks, 'copy_info': chat_copy_info})\n",
    "    return messages_df\n",
    "              \n",
    "\n",
    "def get_code_history(telemetry_data, task_id, last_time):\n",
    "\n",
    "    save_events = [event for event in telemetry_data if event[\"event_type\"] == \"save_code\"]\n",
    "    code_states = []\n",
    "    times = []\n",
    "    task_names = []\n",
    "    time_gaps = []\n",
    "\n",
    "    for save_event in save_events:\n",
    "        code_states.append(save_event[\"code\"])\n",
    "        times.append(save_event[\"timestamp\"])\n",
    "        if save_event[\"task_index\"] == -1:\n",
    "            task_name = all_tasks[tutorial_task_index][\"name\"]\n",
    "        else:\n",
    "            task_name = all_tasks[task_sets[task_id][save_event[\"task_index\"]]][\"name\"]\n",
    "        task_names.append(task_name)\n",
    "    # add last code state, the timestamp is last_time but need to convert to linux time\n",
    "    if len(code_states) > 0:\n",
    "        code_states.append(code_states[-1])\n",
    "        # add 35minutes to first time\n",
    "        final_timestamp =  35*60*1000 + times[0]\n",
    "        times.append(final_timestamp)\n",
    "        task_names.append(task_names[-1])    \n",
    "        # add column time gaps between each code state\n",
    "        # first gap is 0\n",
    "        time_gaps.append(0)\n",
    "        for i in range(1, len(times)):\n",
    "            time_gaps.append((times[i] - times[i-1])/1000)\n",
    "    code_df = pd.DataFrame({'code': code_states, 'times': times, 'task_name': task_names, 'time_gaps': time_gaps})\n",
    "    return code_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get participants data from google form\n",
    "df_participants_form = pd.read_csv('../../data/gpt4_all_participants.csv')\n",
    "df_participants_form = df_participants_form[['First Name', 'Last Name', 'Email (If you are student, you must provide your .edu email here)', 'Email for Amazon gift card compensation', 'Which best describes your programming experience?', 'How proficient are you with Python?', 'How often do you use AI tools for programming (e.g., GitHub Copilot, ChatGPT)?']]\n",
    "df_participants_form['name'] = df_participants_form['First Name'] + \" \" + df_participants_form['Last Name']\n",
    "df_participants_form = df_participants_form.drop(columns=['First Name', 'Last Name'])\n",
    "df_participants_form = df_participants_form.rename(columns={'Email (If you are student, you must provide your .edu email here)': 'email1', 'Email for Amazon gift card compensation': 'email2', 'Which best describes your programming experience?': 'prog_experience', 'How proficient are you with Python?': 'python_experience', 'How often do you use AI tools for programming (e.g., GitHub Copilot, ChatGPT)?': 'ai_experience'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = db.collection('responses').get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DICT_TOOL_USAGE = {\"1\":\"Strongly Disagree\",\"2\":\"Disagree\", \"3\":\"Neutral\", \"4\":\"Agree\", \"5\":\"Strongly Agree\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (pd.DataFrame([x.to_dict() for x in docs])\n",
    "     .dropna(subset=[\"telemetry_data\", \"completed_task_time\", \"date_performed\"])\n",
    "     # remove those with test in name or email\n",
    "     .loc[lambda x: ~x[\"name\"].str.contains(\"test\", case=False)]\n",
    "     .loc[lambda x: ~x[\"email\"].str.contains(\"test\", case=False)]\n",
    "     # if name Subhro Das\n",
    "     .loc[lambda x: ~x[\"name\"].str.contains(\"Subhro Das\", case=False)]\n",
    "     # numb participants\n",
    "     .assign(n_participants = 1)\n",
    "     .assign(completed_task_time = lambda x: [dparser.parse(re.sub(\"\\s*\\([^)]*\\)\", \"\", y), fuzzy=True) for y in x[\"completed_task_time\"]])\n",
    "     .assign(date_performed = lambda x: [dparser.parse(re.sub(\"\\s*\\([^)]*\\)\", \"\", y), fuzzy=True) for y in x[\"date_performed\"]])\n",
    "     .assign(task_duration = lambda x: x.completed_task_time - x.date_performed)\n",
    "     .assign(model = lambda x: [re.match(\"[a-zA-Z]*_[a-zA-Z0-9]*\", x)[0] if re.match(\"[a-zA-Z]*_[a-zA-Z0-9]*\", str(x)) else \"\" for x in x[\"task_id\"]])\n",
    "     .assign(interface = lambda x: [\"autocomplete\" if \"autocomplete\" in y else \"chat\" if \"chat\" in y else \"nomodel\" for y in x[\"model\"]])\n",
    "     .assign(task_id = lambda x: [int(x.split(\"_\")[-2]) for x in x[\"task_id\"]])\n",
    "     .assign(n_tasks_completed = lambda z: [len([x for x in y if x[\"event_type\"] == \"submit_code\" and x[\"completed_task\"] == 1 ]) for y in z[\"telemetry_data\"]])\n",
    "     .assign(n_tasks_attempted = lambda z: [len([x for x in y if x[\"event_type\"] == \"load_task\"]) for y in z[\"telemetry_data\"]])\n",
    "     .assign(n_tasks_skipped = lambda z: [len([x for x in y if x[\"event_type\"] == \"skip_task\"]) for y in z[\"telemetry_data\"]])\n",
    "     .assign(task_completion_durations = lambda x: [get_time_to_completion(y)[0] for y in x['telemetry_data']])\n",
    "     .assign(mean_task_duration = lambda x: [get_time_to_completion(y)[1] for y in x['telemetry_data']])\n",
    "     #  entered_exit_survey - date_performed    \n",
    "     .assign(coding_time = lambda x: [get_coding_time(t, y, z) for t, y, z in zip(x['telemetry_data'], x['entered_exit_survey'], x['date_performed'])])\n",
    "\n",
    "     # code history\n",
    "     .assign(code_history = lambda x: [get_code_history(y, z, t) for y, z, t in zip(x['telemetry_data'], x['task_id'], x['entered_exit_survey'])])\n",
    "\n",
    "     .assign(TLX_frustration = lambda x: x[\"frustration\"].astype(int))\n",
    "     .assign(TLX_performance = lambda x: x[\"performance\"].astype(int))\n",
    "     .assign(TLX_temporal_demand = lambda x: x[\"temporalDemand\"].astype(int))\n",
    "     .assign(TLX_physical_demand = lambda x: x[\"physicalDemand\"].astype(int))\n",
    "     .assign(TLX_effort = lambda x: x[\"effort\"].astype(int))\n",
    "     .assign(TLX_mental_demand = lambda x: x[\"mentalDemand\"].astype(int))\n",
    "     .assign(TLX_total_score = lambda x: x.filter(like=\"TLX\").sum(axis=1) * 5)\n",
    "      \n",
    "      # autocomplete specific\n",
    "     .assign(n_sugg_accepted = lambda z: [get_suggestion_acceptance_rate(y)[0] for y in z[\"telemetry_data\"]])\n",
    "     .assign(n_sugg_shown = lambda z: [get_suggestion_acceptance_rate(y)[1] for y in z[\"telemetry_data\"]])\n",
    "     .assign(sugg_accept_rate = lambda x: x.n_sugg_accepted / x.n_sugg_shown)\n",
    "     .assign(time_spent_verifying = lambda x: [get_time_verifying_suggestion(y) for y in x['telemetry_data']])\n",
    "     # for suggestions requested\n",
    "     .assign(n_sugg_requested = lambda z: [get_suggestions_requested(y)[0] for y in z[\"telemetry_data\"]]) \n",
    "     .assign(n_sugg_accepted_requested = lambda z: [get_suggestions_requested(y)[1] for y in z[\"telemetry_data\"]])\n",
    "     .assign(sugg_accept_rate_requested = lambda x: x.n_sugg_accepted_requested / x.n_sugg_requested)\n",
    "     # acceptance rate for non requested suggestions\n",
    "     .assign(sugg_accept_rate_non_requested = lambda x: (x.n_sugg_accepted - x.n_sugg_accepted_requested) / (x.n_sugg_shown - x.n_sugg_requested))\n",
    "     # get suggestion data\n",
    "     .assign(suggestions_data = lambda x: [get_suggestions_labels(y, z) for y, z in zip(x['telemetry_data'], x['task_id'])])\n",
    "\n",
    "     # chat specific\n",
    "     .assign(n_assistant_response = lambda z: [get_chat_messages_count(y)[0] for y in z[\"telemetry_data\"]])\n",
    "     .assign(n_user_message = lambda z: [get_chat_messages_count(y)[1] for y in z[\"telemetry_data\"]])\n",
    "     .assign(n_copy_code_button = lambda z: [get_chat_messages_count(y)[2] for y in z[\"telemetry_data\"]])\n",
    "     .assign(n_copy_from_chat = lambda z: [get_chat_messages_count(y)[3] for y in z[\"telemetry_data\"]])\n",
    "     .assign(avg_copy_per_response = lambda z: [get_chat_messages_count(y)[4] for y in z[\"telemetry_data\"]])\n",
    "     .assign(chat_history_data = lambda x: [get_chat_history(y, z) for y, z in zip(x['telemetry_data'], x['task_id'])])\n",
    "     # both chat and autocomplete\n",
    "     .assign(aihelpful = lambda x: [int(y) if \"aihelpful\" in x else 0 for y in x[\"aihelpful\"]])\n",
    "     .assign(aiToolTypicalUsage = lambda x: [DICT_TOOL_USAGE[y] if y in DICT_TOOL_USAGE else y for y in x[\"aiToolTypicalUsage\"]])\n",
    "\n",
    "\n",
    "     # task level information\n",
    "     .assign(task_data = lambda x: [get_task_data(y, z) for y, z in zip(x['telemetry_data'], x['task_id'])])\n",
    ")\n",
    "\n",
    "df.loc[[\"model\" in x for x in df[\"model\"]], \"model\"] = \"nomodel\"\n",
    "\n",
    "\n",
    "# FILTERING for quality:\n",
    "# those who have not completed the tutorial\n",
    "print(\"n participants before filtering: \", len(df))\n",
    "df = df.query(\"n_tasks_completed >= 1\").reset_index()\n",
    "print(\"n participants after filtering for tutorial completion: \", len(df))\n",
    "# for each row, see column time_gaps in code_history and count how many times it is higher than 15 mins\n",
    "long_gaps = []\n",
    "for i in range(len(df)):\n",
    "    # count how \n",
    "    gaps_large = len([x for x in df['code_history'][i]['time_gaps'] if x > 60*15 and x < 3000])\n",
    "    long_gaps.append(gaps_large)\n",
    "df['n_long_gaps'] = long_gaps\n",
    "# remove those with long_gaps > 0\n",
    "df = df.query(\"n_long_gaps == 0\").reset_index()\n",
    "print(\"n participants after filtering for long gaps: \", len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# those with interface autocomplete and n_sugg_shown is 0\n",
    "print(\"Autocomplete but no suggestions shown:\")\n",
    "print(len(df[(df['interface'] == 'autocomplete') & (df['n_sugg_shown'] == 0)]['model']))\n",
    "print(\"Chat but never used it:\")\n",
    "print(len(df[(df['interface'] == 'chat') & (df['n_user_message'] == 0)]['model']))\n",
    "print(\"Chat with user message but no assistant response:\")\n",
    "print(len(df[(df['interface'] == 'chat') & (df['n_user_message'] > 0) & (df['n_assistant_response'] == 0)]['model']))\n",
    "print(\"Study completion time under 35 mins\")\n",
    "print(len(df[df['coding_time']/60 < 36]['model']))\n",
    "print(\"Study completion time under 30 mins\")\n",
    "print(len(df[df['coding_time']/60 < 30]['model']))\n",
    "# those above 35 mins\n",
    "print(\"Study completion time over 36 mins\")\n",
    "print(len(df[df['coding_time']/60 > 36]['model']))\n",
    "print(\"Study completion time over 40 mins\")\n",
    "print(len(df[df['coding_time']/60 > 40 ]['model']))\n",
    "print(\"Study completion time over 50 mins\")\n",
    "print(len(df[df['coding_time']/60 > 50 ]['model']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "merge with google form data of participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df\n",
    "df2 = df_participants_form\n",
    "df2_renamed_email2 = df2.rename(columns={'email2': 'email'})\n",
    "df2_renamed_email2 = df2_renamed_email2.drop_duplicates(subset=['email'], keep='first')\n",
    "merge_on_email2 = pd.merge(df1, df2_renamed_email2, on='email', how='left')\n",
    "# Identify rows in df1 that were not matched\n",
    "unmatched = merge_on_email2[merge_on_email2['name_y'].isnull()]\n",
    "\n",
    "# Merge unmatched rows on email1\n",
    "if not unmatched.empty:\n",
    "    df2_renamed_email1 = df2.rename(columns={'email1': 'email'})\n",
    "    merge_on_email1 = pd.merge(unmatched, df2_renamed_email1, on='email', how='left')\n",
    "    \n",
    "    # Combine the two sets of matched rows\n",
    "    final_df = pd.concat([merge_on_email2[~merge_on_email2['name_y'].isnull()], merge_on_email1])\n",
    "\n",
    "    # Drop temporary columns and fill NaNs for unmatched rows\n",
    "    final_df = final_df.drop(columns=['name_y', 'email1', 'email2']).rename(columns={'name_x': 'name'}).reset_index(drop=True)\n",
    "else:\n",
    "    final_df = merge_on_email2.drop(columns=['name_y', 'email1', 'email2']).rename(columns={'name_x': 'name'}).reset_index(drop=True)\n",
    "\n",
    "print(len(final_df))\n",
    "# prog_experience\tpython_experience\tai_experience\tprog_experience_x\tpython_experience_x\tai_experience_x\tprog_experience_y\tpython_experience_y\tai_experience_y\n",
    "# combine properly\n",
    "final_df['prog_experience'] = final_df['prog_experience'].combine_first(final_df['prog_experience_y']).combine_first(final_df['prog_experience_x'])\n",
    "final_df['python_experience'] = final_df['python_experience'].combine_first(final_df['python_experience_y']).combine_first(final_df['python_experience_x'])\n",
    "final_df['ai_experience'] = final_df['ai_experience'].combine_first(final_df['ai_experience_y']).combine_first(final_df['ai_experience_x'])\n",
    "# drop _x and _y\n",
    "final_df = final_df.drop(columns=['prog_experience_x', 'python_experience_x', 'ai_experience_x', 'prog_experience_y', 'python_experience_y', 'ai_experience_y'])\n",
    "\n",
    "# for those with nan in prog_experience, try to match based on name in df_participants_form\n",
    "names_df_participants_form = df_participants_form['name'].values.tolist()\n",
    "for i in range(len(final_df)):\n",
    "    if pd.isna(final_df['prog_experience'][i]):\n",
    "        # print row i\n",
    "        name = final_df.iloc[i]['name'][0]\n",
    "        closest_name, score = process.extractOne(name, names_df_participants_form)\n",
    "        if score > 0:\n",
    "            row = df_participants_form[df_participants_form['name'] == closest_name]\n",
    "            final_df.at[i, 'prog_experience'] = row['prog_experience'].values[0]\n",
    "            final_df.at[i, 'python_experience'] = row['python_experience'].values[0]\n",
    "            final_df.at[i, 'ai_experience'] = row['ai_experience'].values[0]\n",
    "\n",
    "\n",
    "\"\"\" with pd.option_context('display.max_rows', None,\n",
    "                       'display.max_columns', None,\n",
    "                       'display.precision', 3,\n",
    "                       ):\n",
    "    print(final_df['prog_experience']) \"\"\"\n",
    "\n",
    "# drop merge_key name and email\n",
    "duplicated_names = final_df['name'].duplicated(keep=False)\n",
    "duplicated_emails = final_df['email'].duplicated(keep=False)\n",
    "\n",
    "# A row is marked for keeping if it is not duplicated in either 'name' or 'email'\n",
    "# We invert the duplicated flags with ~ and use | to combine them, marking rows duplicated in either column\n",
    "rows_to_keep = ~(duplicated_names | duplicated_emails)\n",
    "\n",
    "# Filter the DataFrame based on rows_to_keep\n",
    "final_df_unique = final_df[rows_to_keep]\n",
    "\n",
    "final_df = final_df.drop(columns=['name', 'email'])\n",
    "# how many nan in prog_experience\n",
    "print(final_df['prog_experience'].isna().sum())\n",
    "print(len(final_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_pickle('../../data/final_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Participant Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get participants data from google form\n",
    "df_participants_form = pd.read_csv('../../data/gpt4_all_participants.csv')\n",
    "df_participants_form['name'] = df_participants_form['First Name'] + \" \" + df_participants_form['Last Name']\n",
    "df_participants_form = df_participants_form.drop(columns=['First Name', 'Last Name'])\n",
    "df_participants_form = df_participants_form.rename(columns={'Email (If you are student, you must provide your .edu email here)': 'email1', 'Email for Amazon gift card compensation': 'email2', 'Which best describes your programming experience?': 'prog_experience', 'How proficient are you with Python?': 'python_experience', 'How often do you use AI tools for programming (e.g., GitHub Copilot, ChatGPT)?': 'ai_experience'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example dataframes\n",
    "# df1 = pd.DataFrame({'name': ['Name1', 'Name2'], 'email1': ['email1@example.com', 'email2@example.com'], 'email2': ['email3@example.com', 'email4@example.com']})\n",
    "# df2 = pd.DataFrame({'name': ['Name3', 'Name1'], 'email': ['email5@example.com', 'email1@example.com']})\n",
    "\n",
    "df1 = df_participants_form\n",
    "df2 = df \n",
    "\n",
    "# Convert df2['email'] to a list for efficient searching\n",
    "emails_in_df2 = df2['email'].tolist()\n",
    "names_in_df2 = df2['name'].tolist()\n",
    "\n",
    "# Define a function to apply across df1 to check the conditions\n",
    "def match_row(row):\n",
    "    return (row['name'] in names_in_df2) or (row['email1'] in emails_in_df2) or (row['email2'] in emails_in_df2)\n",
    "\n",
    "# Apply this function to filter df1\n",
    "filtered_df1 = df1[df1.apply(match_row, axis=1)]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of each gender value\n",
    "gender_percentages = filtered_df1['Gender'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Print the result\n",
    "print(gender_percentages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentages = filtered_df1['Occupation'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Print the result\n",
    "print(percentages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentages = filtered_df1['prog_experience'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Print the result\n",
    "print(percentages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentages = filtered_df1['python_experience'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Print the result\n",
    "print(percentages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentages = filtered_df1['ai_experience'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Print the result\n",
    "print(percentages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset level statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'number of participants {len(final_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentages = final_df['model'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Print the result\n",
    "print(percentages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentages = final_df['model'].value_counts() \n",
    "\n",
    "# Print the result\n",
    "print(percentages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentages = sum(final_df['n_tasks_completed'])\n",
    "\n",
    "# Print the result\n",
    "print(percentages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['n_tasks_completed'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentages = sum(final_df['n_sugg_shown'])\n",
    "\n",
    "# Print the result\n",
    "print(percentages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentages = sum(final_df['n_sugg_accepted']) \n",
    "\n",
    "# Print the result\n",
    "print(percentages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentages = sum(final_df['n_sugg_accepted']) / sum(final_df['n_sugg_shown'])\n",
    "\n",
    "# Print the result\n",
    "print(percentages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentages = sum(final_df['n_assistant_response'])\n",
    "\n",
    "# Print the result\n",
    "print(percentages)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
