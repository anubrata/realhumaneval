{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ceef6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "df = get_dataframe(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b3e519",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['model'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8262b9fd",
   "metadata": {},
   "source": [
    "Explanation of the different columns in df:\n",
    "\n",
    "model: condition identifier\n",
    "\n",
    "model_name: model identifier\n",
    "\n",
    "task_id: task set identifier\n",
    "\n",
    "interface: interface identifier autocomplete, chat or nomodel\n",
    "\n",
    "aihelpful: how helpful the AI was (rating int)\n",
    "\n",
    "mean_task_duration: mean time taken to complete a task\n",
    "\n",
    "n_tasks_completed: number of tasks completed\n",
    "\n",
    "sugg_accept_rate: average suggestion acceptance rate\n",
    "\n",
    "n_sugg_requested: number of suggestions requested\n",
    "\n",
    "sugg_accept_rate_requested: suggestion acceptance rate for requested suggestions\n",
    "\n",
    "sugg_accept_rate_non_requested: suggestion acceptance rate for non-requested suggestions\n",
    "\n",
    "n_assistant_response: number of chatbot messages\n",
    "\n",
    "n_copy_code_button: times the copy code button was clicked\n",
    "\n",
    "n_copy_from_chat: times code was copied from chat of AI\n",
    "\n",
    "avg_copy_per_response: average number of times code was copied per chatbot message\n",
    "\n",
    "prog_experience: programming experience from google form\n",
    "\n",
    "python_experience: python experience from google form\n",
    "\n",
    "ai_experience: AI experience from google form\n",
    "\n",
    "task_data: for all tasks attempted, the final code that was submitted, whether the task was \n",
    "completed, and the time taken to complete the task, and if skipped\n",
    "\n",
    "code_history: for all tasks attempted, the code history of the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "593f5ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 20})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8971dacf",
   "metadata": {},
   "source": [
    "# Section 4\n",
    "\n",
    "Visualizing results from two papers: *Code llama: Open foundation models for code* and *Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9b0f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "plt.figure(figsize=(13,6))\n",
    "\n",
    "\n",
    "color1 = (0.2,0.4,0.2,0.8) \n",
    "color2 = (0.1,0.4,0.2,1) \n",
    "color3 = (0.2,0.4,0.7,0.8) \n",
    "color4 = (0.2,0.4,0.7,1) \n",
    "color5 = (0.6,0.2,0.6,0.8) \n",
    "color6 = (0.8,0.2,0.6,1) \n",
    "color7 = (0.8,0.8,0.2,1)\n",
    "colors = [color1, color2, color3, color4, color5, color6, color7]\n",
    "\n",
    "\n",
    "\n",
    "species = (\"HumanEval\", \"MBPP\")\n",
    "penguin_means = {\n",
    "    'CodeLlama7b': (33.5, 41.4),\n",
    "    'CodeLlama7b (chat)': (34.8, 44.4),\n",
    "    'CodeLlama34b': (48.8, 55.0),\n",
    "    'CodeLlama34b (chat)': (41.5, 57.0),\n",
    "    'GPT-3.5 ': (80.34, 81.03),\n",
    "    'GPT-3.5 (chat)': (77.0, 82.91),\n",
    "    'GPT-4 (chat)': (90.2, 85.7),\n",
    "}\n",
    "\n",
    "\n",
    "x = np.arange(len(species))  # the label locations\n",
    "width = 0.15  # the width of the bars\n",
    "multiplier = 0\n",
    "\n",
    "\n",
    "for attribute, measurement in penguin_means.items():\n",
    "    offset = width * multiplier\n",
    "    hatch_pattern = '/' if 'chat' in attribute else ''  # Apply hatch pattern if 'chat' is in the label\n",
    "    rects = plt.bar(x + offset, measurement, width, label=attribute, color=colors[multiplier], hatch=hatch_pattern)\n",
    "    multiplier += 1\n",
    "\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "plt.ylabel('Pass@1')\n",
    "plt.xticks(x + 2.5*width, species)\n",
    "# plt.legend(loc='bottom', ncols=3)\n",
    "plt.ylim(20, 91)\n",
    "\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.1),\n",
    "          fancybox=True, shadow=True, ncol=3)\n",
    "plt.savefig('figures/benchmark.pdf', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cde3d0",
   "metadata": {},
   "source": [
    "# Section 5 stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20297b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'number of participants {len(df)}')\n",
    "\n",
    "print(\"Percentage of participants by condition:\", df['model'].value_counts(normalize=True) * 100)\n",
    "print(\"Count of participants by condition:\", df['model'].value_counts())\n",
    "print('Total number of tasks completed', sum(df['n_tasks_completed']))\n",
    "print(\"Summary statistics for the number of tasks completed:\",df['n_tasks_completed'].describe())\n",
    "\n",
    "print('total number of suggestions shown',  sum(df['n_sugg_shown']))\n",
    "print('total number of suggestions accepted', sum(df['n_sugg_accepted']) )\n",
    "print('percentage of suggestions accepted', sum(df['n_sugg_accepted']) / sum(df['n_sugg_shown']))\n",
    "\n",
    "\n",
    "print('total number of assistant responses', sum(df['n_assistant_response']))\n",
    "print('total number of copied messages from responses', sum(df['n_copy_from_chat']))\n",
    "df['copied_per_response'] = np.where(df['avg_copy_per_response'] > 0, 1, 0)\n",
    "print('average copy rate from chat response', np.nanmean(df['copied_per_response']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9d31d5",
   "metadata": {},
   "source": [
    "# Section 5.1\n",
    "\n",
    "\n",
    "### Task duration and tasks completed bar plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aba88bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore nans\n",
    "mean_duration_by_model = df.groupby('model_name')['mean_task_duration'].mean()\n",
    "stderr_duration_by_model = df.groupby('model_name')['mean_task_duration'].sem()\n",
    "task_duration_values_by_model = df.groupby('model_name')['mean_task_duration'].apply(list)\n",
    "\n",
    "\n",
    "task_completion_time_df = pd.DataFrame({\n",
    "    'mean': mean_duration_by_model,\n",
    "    'se': stderr_duration_by_model,\n",
    "    'values': task_duration_values_by_model\n",
    "})\n",
    "\n",
    "\n",
    "mean_duration_by_model = df.groupby('model_name')['n_tasks_completed'].mean()\n",
    "stderr_duration_by_model = df.groupby('model_name')['n_tasks_completed'].sem()\n",
    "tasks_completed_values_by_model = df.groupby('model_name')['n_tasks_completed'].apply(list)\n",
    "\n",
    "tasks_completed_df = pd.DataFrame({\n",
    "    'mean': mean_duration_by_model,\n",
    "    'se': stderr_duration_by_model,\n",
    "    'values': tasks_completed_values_by_model\n",
    "})\n",
    "\n",
    "task_completion_time_df = task_completion_time_df.reindex(['No LLM', 'CodeLlama7b', 'CodeLlama7b (chat)', 'CodeLlama34b', 'CodeLlama34b (chat)', 'GPT-3.5', 'GPT-3.5 (chat)','GPT-4o (chat)'])\n",
    "tasks_completed_df = tasks_completed_df.reindex(['No LLM', 'CodeLlama7b', 'CodeLlama7b (chat)', 'CodeLlama34b', 'CodeLlama34b (chat)', 'GPT-3.5', 'GPT-3.5 (chat)', 'GPT-4o (chat)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5541da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "\n",
    "color0 = (0,0,0,0.8)\n",
    "color1 = (0.2,0.4,0.2,0.8) \n",
    "color2 = (0.1,0.4,0.2,1) \n",
    "color3 = (0.2,0.4,0.7,0.8) \n",
    "color4 = (0.2,0.4,0.7,1) \n",
    "color5 = (0.6,0.2,0.6,0.8) \n",
    "color6 = (0.8,0.2,0.6,1) \n",
    "# yellow\n",
    "color7 = (0.8,0.8,0.2,1)\n",
    "colors = [color0, color1, color2, color3, color4, color5, color6, color7]\n",
    "\n",
    "x = 0 # the label locations\n",
    "width = 0.17  # the width of the bars\n",
    "multiplier = 0\n",
    "\n",
    "\n",
    "for i in range(len(task_completion_time_df)):\n",
    "    offset = width * multiplier  \n",
    "    name = task_completion_time_df.iloc[i].name\n",
    "    hatch_pattern = '/' if 'chat' in name else ''  # Apply hatch pattern if 'chat' is in the label\n",
    "    measurement = task_completion_time_df.iloc[i]['mean']\n",
    "    stderr = task_completion_time_df.iloc[i]['se']\n",
    "    rects = plt.bar(x + offset , measurement, width - 0.02, label=name, color=colors[multiplier],hatch=hatch_pattern)\n",
    "    # add stderr\n",
    "    plt.errorbar(x + offset , measurement, stderr, fmt='none', ecolor='black', capsize=5, capthick=2)\n",
    "    # get percentage improvement in measurement over No LLM\n",
    "    improvement = (measurement - task_completion_time_df.loc['No LLM']['mean']) / task_completion_time_df.loc['No LLM']['mean'] * 100\n",
    "    # add text\n",
    "    null_values = tasks_completed_df.loc['No LLM']['values']\n",
    "    alt_values = tasks_completed_df.loc[name]['values']\n",
    "    t, p = stats.ttest_ind(null_values, alt_values)\n",
    "    sign = \"+\" if improvement >= 0 else \"-\"\n",
    "    if name != \"No LLM\":\n",
    "        if p < 0.05:\n",
    "            plt.text(x + offset, measurement + 45, f\"{sign}{abs(improvement):.0f}%*\", ha='center', va='bottom', fontsize=14)\n",
    "        else:\n",
    "            plt.text(x + offset, measurement + 45, f\"{sign}{abs(improvement):.0f}%\", ha='center', va='bottom', fontsize=14)\n",
    "    multiplier += 1\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "plt.ylabel('Task Duration (s)')\n",
    "plt.xlabel(\"Condition\")\n",
    "# plt.legend(loc='bottom', ncols=3)\n",
    "plt.ylim(100, 520)\n",
    "plt.xticks([0], [''])\n",
    "\n",
    "plt.savefig('figures/task_duration_barplot_no_legend.pdf', bbox_inches='tight')\n",
    "\n",
    "#plt.legend(loc='upper left', bbox_to_anchor=(1, 1), fancybox=True, shadow=True, ncol=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b5f57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "x = 0 # the label locations\n",
    "width = 0.17  # the width of the bars\n",
    "multiplier = 0\n",
    "\n",
    "\n",
    "for i in range(len(tasks_completed_df)):\n",
    "    offset = width * multiplier  \n",
    "    name = tasks_completed_df.iloc[i].name\n",
    "    hatch_pattern = '/' if 'chat' in name else ''  # Apply hatch pattern if 'chat' is in the label\n",
    "    measurement = tasks_completed_df.iloc[i]['mean']\n",
    "    stderr = tasks_completed_df.iloc[i]['se']\n",
    "    rects = plt.bar(x + offset , measurement, width - 0.02, label=name, color=colors[multiplier],hatch=hatch_pattern)\n",
    "    # add stderr\n",
    "    plt.errorbar(x + offset , measurement, stderr, fmt='none', ecolor='black', capsize=5, capthick=2)\n",
    "    # get percentage improvement in measurement over No LLM\n",
    "    improvement = (measurement - tasks_completed_df.loc['No LLM']['mean']) / tasks_completed_df.loc['No LLM']['mean'] * 100\n",
    "    # add text\n",
    "    # perform statistcal test\n",
    "\n",
    "    sign = \"+\" if improvement >= 0 else \"-\"\n",
    "    if name != \"No LLM\":\n",
    "        plt.text(x + offset, measurement +0.5, f\"{sign}{abs(improvement):.0f}%\", ha='center', va='bottom', fontsize=14)\n",
    "    multiplier += 1\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "plt.ylabel('Tasks Completed')\n",
    "plt.xlabel(\"Condition\")\n",
    "#plt.legend(loc='bottom')\n",
    "plt.ylim(0, 5.2)\n",
    "plt.xticks([0], [''])\n",
    "plt.yticks([0, 1, 2, 3, 4, 5])\n",
    "#plt.legend(loc='upper left', bbox_to_anchor=(1, 1), fancybox=True, shadow=True, ncol=1)\n",
    "plt.savefig('figures/tasks_completed_barplot.pdf', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4ec9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "x = 0 # the label locations\n",
    "width = 0.17  # the width of the bars\n",
    "multiplier = 0\n",
    "\n",
    "\n",
    "for i in range(len(tasks_completed_df)):\n",
    "    offset = width * multiplier  \n",
    "    name = tasks_completed_df.iloc[i].name\n",
    "    hatch_pattern = '/' if 'chat' in name else ''  # Apply hatch pattern if 'chat' is in the label\n",
    "    measurement = tasks_completed_df.iloc[i]['mean']\n",
    "    stderr = tasks_completed_df.iloc[i]['se']\n",
    "    rects = plt.bar(x + offset , measurement, width - 0.02, label=name, color=colors[multiplier],hatch=hatch_pattern)\n",
    "    # add stderr\n",
    "    plt.errorbar(x + offset , measurement, stderr, fmt='none', ecolor='black', capsize=5, capthick=2)\n",
    "    # get percentage improvement in measurement over No LLM\n",
    "    improvement = (measurement - tasks_completed_df.loc['No LLM']['mean']) / tasks_completed_df.loc['No LLM']['mean'] * 100\n",
    "    # add text\n",
    "    # perform statistcal test\n",
    "\n",
    "    sign = \"+\" if improvement >= 0 else \"-\"\n",
    "    if name != \"No LLM\":\n",
    "        plt.text(x + offset, measurement +0.5, f\"{sign}{abs(improvement):.0f}%\", ha='center', va='bottom', fontsize=14)\n",
    "    multiplier += 1\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "plt.ylabel('Tasks Completed')\n",
    "plt.xlabel(\"Condition\")\n",
    "#plt.legend(loc='bottom')\n",
    "plt.ylim(0, 5.2)\n",
    "plt.xticks([0], [''])\n",
    "plt.yticks([0, 1, 2, 3, 4, 5])\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1, 1), fancybox=True, shadow=True, ncol=1)\n",
    "plt.savefig('figures/tasks_completed_barplot_withlegend.pdf', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc26811",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pointplot(x=\"zscore_mean_task_duration\", y=\"model_size\", data=df, linestyles=\"\", errorbar=\"se\")\n",
    "plt.ylabel(\"\")\n",
    "plt.xlabel(r'$\\Delta$ in Avg Task Duration ($\\downarrow$ better)')\n",
    "#plt.xlim(-1, 1)\n",
    "plt.xlim(-120,100)\n",
    "plt.yticks([0,1,2,3,4], ['GPT-3.5', 'CodeLlama-34b', 'CodeLlama-7b', 'GPT-4', 'No LLM'])#plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.legend([],[], frameon=False)\n",
    "plt.savefig('figures/mean_task_duration.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f59f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relabel(value):\n",
    "    \n",
    "    if value == 'autocomplete':\n",
    "        return \"Autocomplete\"\n",
    "    elif value == 'chat':\n",
    "        return \"Chat\"\n",
    "    else:\n",
    "        return \"No LLM\"\n",
    "\n",
    "df['interface_clean'] = df['interface'].apply(relabel)\n",
    "\n",
    "\n",
    "sns.pointplot(x=\"zscore_n_tasks_completed\", y=\"model_size\", data=df, linestyles=\"\")\n",
    "plt.ylabel(\"\")\n",
    "plt.xlabel(r'$\\Delta$ in Num Tasks Completed ($\\uparrow$ better)')\n",
    "plt.xlim(-2,2)\n",
    "plt.tick_params(left = False , labelleft = False ) \n",
    "plt.savefig('figures/n_tasks_completed.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9da555",
   "metadata": {},
   "source": [
    "### Time spent based on coding task category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "261b903a",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_task_level_dfs = []\n",
    "\n",
    "import ast\n",
    "\n",
    "for row in df.itertuples():\n",
    "    #temp = ast.literal_eval(row.task_data)\n",
    "    try:\n",
    "        temp = row.task_data\n",
    "\n",
    "        df_temp = pd.DataFrame(temp.values()).assign(model=row.model, interface=row.interface, model_size=row.model_size, \n",
    "                                                            task_set=temp.keys())\n",
    "        list_task_level_dfs.append(df_temp)\n",
    "    except:\n",
    "        print(row.task_data[\"-1\"])\n",
    "        continue\n",
    "    \n",
    "    \n",
    "df_task_level = pd.concat(list_task_level_dfs, ignore_index=True).assign(has_ai = lambda x: x.model != \"nomodel\").query(\"time_in_task < 30*60 and name != 'event_scheduler'\").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2f667cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_display_order = [\"sum_product\", \n",
    "                      \"t_test\", \"table_transform_named\", \"table_transform_unnamed1\", \"table_transform_unnamed2\", \n",
    "                      \"tokenizer\", \"calculator\", \"login_authenticator\", \"retriever\",\n",
    "                      \"even_odd_count\", \"triple_sum_to_zero\", \"encode_message\", \"is_bored\", \"is_multiply_prime\", \"count_nums\", \"order_by_points\"]\n",
    "\n",
    "df_task_level[\"ordered_name\"] = pd.Categorical(df_task_level[\"name\"], categories=task_display_order, ordered=True)\n",
    "df_task_level[\"task_category\"] = df_task_level[\"name\"].map(dict(zip(task_display_order, [\"tutorial\"]+[\"data_manipulation\"]*4+[\"edit_code\"]*4+[\"puzzles\"]*7)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4d2bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pointplot(y=\"task_category\", x=\"time_in_task\", hue=\"has_ai\", dodge=0.25, data=df_task_level.query(\"task_category != 'tutorial'\"), linestyles=\"\", errorbar=\"se\")\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.xlabel(\"Avg task duration (s)\", fontsize=18)\n",
    "plt.ylabel(\"\")\n",
    "plt.yticks([0, 1, 2], ['Algorithmic Problems', 'Data Manipulation', 'Edit/Augment Code'])\n",
    "plt.legend(title='LLM-assisted')\n",
    "plt.savefig('figures/task_level_duration_categories.pdf', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb9a153",
   "metadata": {},
   "source": [
    "# Section 5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cee971b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add column to data called interface_plus_gpt4\n",
    "df['interface_plus_gpt4'] = df['interface'] \n",
    "df.loc[df['model_clean_name'] == 'GPT-4o (chat)', 'interface_plus_gpt4'] = 'autocomplete'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca57fbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add one more line in the plot with no data\n",
    "plt.legend([],[], frameon=False)\n",
    "sns.pointplot(x=\"sugg_accept_rate\", y=\"model_clean_name\", data=df.query(\"interface_plus_gpt4 == 'autocomplete'\"), linestyles=\"\", errorbar=\"se\")\n",
    "\n",
    "# add one more line in the plot with no data\n",
    "plt.ylabel(\"\")\n",
    "#plt.yticks([0,1,2], ['GPT-3.5', 'CodeLlama-34b', 'CodeLlama-7b'])\n",
    "plt.xlabel(r'Autocomplete: % Suggestion Accepted')\n",
    "plt.xlim(0,0.2)\n",
    "plt.tick_params(left = False , labelleft = False ) \n",
    "plt.savefig('figures/num_sugg_accepted.pdf', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99eb653f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pointplot(x=\"avg_copy_per_response\", y=\"model_clean_name\", data=df.query(\"interface == 'chat'\"), linestyles=\"\", errorbar=\"se\")\n",
    "plt.ylabel(\"\")\n",
    "#plt.xlabel(\"Mean Task Duration\")\n",
    "plt.xlabel(r'Chat: % Code Copied')\n",
    "plt.xlim(0,0.5)\n",
    "plt.yticks([0,1,2,3], ['GPT-3.5', 'CodeLlama-34b', 'CodeLlama-7b', 'GPT-4o'])\n",
    "plt.savefig('figures/num_code_copied.pdf', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba3fe17",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,14))\n",
    "sns.violinplot(x=\"zscore_mean_task_duration\", y=\"model_clean_name\", data=df, linestyles=\"\", errorbar=\"se\")\n",
    "plt.ylabel(\"\")\n",
    "plt.xlabel(r'$\\Delta$ in Avg Task Duration ($\\downarrow$ better)')\n",
    "#plt.xlim(-1, 1)\n",
    "#plt.xlim(-120,100)\n",
    "#plt.yticks([0,1,2,3], ['GPT-3.5', 'CodeLlama-34b', 'CodeLlama-7b', 'No LLM'])#plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.legend([],[], frameon=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20104b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relabel(value):\n",
    "    \n",
    "    if value == 'autocomplete':\n",
    "        return \"Autocomplete\"\n",
    "    elif value == 'chat':\n",
    "        return \"Chat\"\n",
    "    else:\n",
    "        return \"No AI\"\n",
    "\n",
    "df['interface_clean'] = df['interface'].apply(relabel)\n",
    "\n",
    "\n",
    "sns.pointplot(x=\"aihelpful\", y=\"model_clean_name\", data=df.query(\"interface == 'chat'\"), linestyles=\"\",  errorbar=\"se\")\n",
    "plt.xlim(2,10)\n",
    "plt.xlabel(r'AI Helpfulness($\\uparrow$)')\n",
    "plt.ylabel(\"\")\n",
    "\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51ec30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relabel(value):\n",
    "    \n",
    "    if value == 'autocomplete':\n",
    "        return \"Autocomplete\"\n",
    "    elif value == 'chat':\n",
    "        return \"Chat\"\n",
    "    else:\n",
    "        return \"No AI\"\n",
    "\n",
    "df['interface_clean'] = df['interface'].apply(relabel)\n",
    "\n",
    "\n",
    "sns.pointplot(x=\"aihelpful\", y=\"model_size\", data=df.query(\"interface != 'nomodel'\"), linestyles=\"\", hue=\"interface_clean\", errorbar=\"se\")\n",
    "plt.ylabel(\"\")\n",
    "plt.xlim(0,8)\n",
    "plt.xlabel(r'AI Helpfulness($\\uparrow$)')\n",
    "plt.tick_params(left = False , labelleft = False ) \n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.savefig('figures/ai_helpful.pdf', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9b65e2",
   "metadata": {},
   "source": [
    "# Stat tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d020f94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(smf.ols(f\"zscore_n_tasks_completed ~ C(model_size, Treatment(reference='nomodel'))\", data=df).fit().summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04427d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(smf.ols(f\"zscore_mean_task_duration ~ C(model_size, Treatment(reference='nomodel'))\", data=df).fit().summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516cc3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(smf.ols(f\"sugg_accept_rate ~ C(model, Treatment(reference='autocomplete_llama34'))\", data=df.query(\"interface=='autocomplete'\")).fit().summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4573d36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(smf.ols(f\"avg_copy_per_response ~ C(model, Treatment(reference='chat_llama7'))\", data=df.query(\"interface=='chat'\")).fit().summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0003f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(smf.ols(f\"aihelpful ~ C(interface, Treatment(reference='autocomplete'))\", data=df.query(\"interface!='nomodel'\")).fit().summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a8e910a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pvals = []\n",
    "fit = smf.ols(f\"zscore_n_tasks_completed ~ C(model_size, Treatment(reference='nomodel'))\", data=df).fit()\n",
    "pvals.append(fit.pvalues[1])\n",
    "pvals.append(fit.pvalues[2])\n",
    "pvals.append(fit.pvalues[3])\n",
    "fit = smf.ols(f\"zscore_mean_task_duration ~ C(model_size, Treatment(reference='nomodel'))\", data=df).fit()\n",
    "pvals.append(fit.pvalues[1]) # significant\n",
    "pvals.append(fit.pvalues[2])\n",
    "pvals.append(fit.pvalues[3])\n",
    "fit = smf.ols(f\"sugg_accept_rate ~ C(model, Treatment(reference='autocomplete_llama34'))\", data=df.query(\"interface=='autocomplete'\")).fit()\n",
    "pvals.append(fit.pvalues[1])#significant\n",
    "pvals.append(fit.pvalues[2]) \n",
    "fit = smf.ols(f\"avg_copy_per_response ~ C(model, Treatment(reference='chat_llama7'))\", data=df.query(\"interface=='chat'\")).fit()\n",
    "pvals.append(fit.pvalues[1])\n",
    "pvals.append(fit.pvalues[2])\n",
    "fit = smf.ols(f\"aihelpful ~ C(interface, Treatment(reference='autocomplete'))\", data=df.query(\"interface!='nomodel'\")).fit()\n",
    "pvals.append(fit.pvalues[1]) # significant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e23fd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels\n",
    "\n",
    "statsmodels.stats.multitest.multipletests(pvals, method=\"fdr_bh\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
